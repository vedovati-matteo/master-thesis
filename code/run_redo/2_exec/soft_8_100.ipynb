{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "data = datasets.load_breast_cancer()\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import jax.numpy as jnp\n",
    "from jax import grad as fgrad\n",
    "from jax import jit, vmap, jacfwd, jacrev, device_put\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import grad\n",
    "from scipy.integrate import solve_ivp\n",
    "np.set_printoptions(suppress=True)\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "import jax\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "CUDA = torch.cuda.is_available()\n",
    "mse = nn.MSELoss(reduction='none')\n",
    "NUMERICAL_METHOD = 'RK45'\n",
    "\n",
    "seed = 23\n",
    "\n",
    "random.seed(seed)\n",
    "# Set seed for NumPy\n",
    "np.random.seed(seed)\n",
    "# Set seed for PyTorch\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU\n",
    "# Set seed for JAX\n",
    "key = jax.random.PRNGKey(seed)\n",
    "\n",
    "def select_n_points(data_X, data_y, n):\n",
    "    # Check if n is even\n",
    "    if n % 2 != 0:\n",
    "        raise ValueError(\"n must be an even number to select equal points from both classes.\")\n",
    "\n",
    "    # Separate the data into two classes: +1 and -1\n",
    "    class_pos = data_X[data_y == 1]  # Class +1\n",
    "    class_neg = data_X[data_y == -1]  # Class -1\n",
    "\n",
    "    # Check if enough points exist in both classes\n",
    "    if len(class_pos) < n // 2 or len(class_neg) < n // 2:\n",
    "        raise ValueError(\"Not enough data points in one or both classes to select n points.\")\n",
    "\n",
    "    # Randomly select n//2 points from each class\n",
    "    selected_pos = class_pos[np.random.choice(len(class_pos), n // 2, replace=False)]\n",
    "    selected_neg = class_neg[np.random.choice(len(class_neg), n // 2, replace=False)]\n",
    "\n",
    "    # Combine the selected points and their labels\n",
    "    selected_X = np.vstack((selected_pos, selected_neg))\n",
    "    selected_y = np.hstack((np.ones(n // 2), -np.ones(n // 2)))\n",
    "\n",
    "    # Shuffle the selected points and labels\n",
    "    selected_X, selected_y = shuffle(selected_X, selected_y, random_state=42)\n",
    "\n",
    "    return selected_X, selected_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.data \n",
    "y = data.target\n",
    "feature_names = data.feature_names\n",
    "\n",
    "X = pd.DataFrame(X, columns=feature_names) \n",
    "# Select only the desired features\n",
    "data_X = X[['mean radius', 'mean concave points', 'worst radius', 'mean perimeter', 'worst concave points', 'mean area', 'worst area', 'mean concavity']].to_numpy()\n",
    "\n",
    "# Encode y to be +1 and -1\n",
    "le = LabelEncoder()\n",
    "data_y = le.fit_transform(y)\n",
    "data_y[data_y == 0] = -1  # Replace 0 with -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 50\n",
    "selected_X, selected_y = select_n_points(data_X, data_y, n)\n",
    "scaler = StandardScaler()\n",
    "selected_X = scaler.fit_transform(selected_X)\n",
    "_, ax = plt.subplots()\n",
    "scatter = ax.scatter(selected_X[:, 0], selected_X[:, 1], c=selected_y)\n",
    "ax.set(xlabel='mean radius', ylabel='mean concave points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the plotting function\n",
    "def plot_svm_decision_boundary(X, y, w, b, feature_x_idx, feature_y_idx):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    # Plot the two classes\n",
    "    plt.scatter(X[y > 0][:, feature_x_idx], X[y > 0][:, feature_y_idx], c='b', label='Malignant')\n",
    "    plt.scatter(X[y < 0][:, feature_x_idx], X[y < 0][:, feature_y_idx], c='r', label='Benign')\n",
    "\n",
    "    # Plot decision boundary\n",
    "    x_min, x_max = X[:, feature_x_idx].min() - 1, X[:, feature_x_idx].max() + 1\n",
    "    y_min, y_max = X[:, feature_y_idx].min() - 1, X[:, feature_y_idx].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
    "                         np.arange(y_min, y_max, 0.02))\n",
    "    \n",
    "    # Use only the selected features for the decision boundary\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    Z = np.dot(grid, w[[feature_x_idx, feature_y_idx]]) + b\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    plt.contour(xx, yy, Z, colors='k', levels=[0], alpha=0.5, linestyles=['-'])\n",
    "    plt.contour(xx, yy, Z, colors='k', levels=[-1, 1], alpha=0.5, linestyles=['--'])\n",
    "    \n",
    "    plt.xlabel(f'mean radius')\n",
    "    plt.ylabel(f'mean concave points')\n",
    "    plt.legend()\n",
    "    #plt.title('Soft Margin SVM Decision Boundary')\n",
    "    plt.show()\n",
    "    \n",
    "# Define the modified plotting function to plot two decision boundaries\n",
    "def plot_svm_decision_boundary_confrontation(X, y, w_1, b_1, w_2, b_2, feature_x_idx, feature_y_idx):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    # Plot the two classes\n",
    "    plt.scatter(X[y > 0][:, feature_x_idx], X[y > 0][:, feature_y_idx], c='b', label='Malignant')\n",
    "    plt.scatter(X[y < 0][:, feature_x_idx], X[y < 0][:, feature_y_idx], c='r', label='Benign')\n",
    "\n",
    "    # Create mesh grid for decision boundaries\n",
    "    x_min, x_max = X[:, feature_x_idx].min() - 1, X[:, feature_x_idx].max() + 1\n",
    "    y_min, y_max = X[:, feature_y_idx].min() - 1, X[:, feature_y_idx].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
    "                         np.arange(y_min, y_max, 0.02))\n",
    "    \n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    \n",
    "    # First model's decision boundary (Ground Truth) in orange with highlight effect (thicker and transparent)\n",
    "    Z1 = np.dot(grid, w_1[[feature_x_idx, feature_y_idx]]) + b_1\n",
    "    Z1 = Z1.reshape(xx.shape)\n",
    "    ground_truth = plt.contour(xx, yy, Z1, colors='purple', levels=[0], alpha=0.3, linewidths=5, linestyles=['-'])\n",
    "    plt.contour(xx, yy, Z1, colors='purple', levels=[-1, 1], alpha=0.3, linewidths=3, linestyles=['--'])\n",
    "    \n",
    "    # Second model's decision boundary (ODE Solution) in purple\n",
    "    Z2 = np.dot(grid, w_2[[feature_x_idx, feature_y_idx]]) + b_2\n",
    "    Z2 = Z2.reshape(xx.shape)\n",
    "    ode_solution = plt.contour(xx, yy, Z2, colors='green', levels=[0], alpha=0.8, linewidths=2, linestyles=['-'])\n",
    "    plt.contour(xx, yy, Z2, colors='green', levels=[-1, 1], alpha=0.8, linewidths=1.5, linestyles=['--'])\n",
    "    \n",
    "    # Add the two main lines to the legend\n",
    "    h1, _ = ground_truth.legend_elements()\n",
    "    h2, _ = ode_solution.legend_elements()\n",
    "    legend_elements = [h1[0], h2[0], \n",
    "                       plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='b', markersize=10),\n",
    "                       plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='r', markersize=10)]\n",
    "    legend_labels = ['CVX Solution', 'ODE Solution', 'Malignant', 'Benign']\n",
    "    plt.legend(legend_elements, legend_labels, loc='upper right')\n",
    "    \n",
    "    # Add labels, title, and show the plot\n",
    "    plt.xlabel(f'mean radius')\n",
    "    plt.ylabel(f'mean concave points')\n",
    "    #plt.title('SVM Decision Boundaries')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_svm_decision_boundary_confrontation_PINN(X, y, w_1, b_1, w_2, b_2, feature_x_idx, feature_y_idx):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    # Plot the two classes\n",
    "    plt.scatter(X[y > 0][:, feature_x_idx], X[y > 0][:, feature_y_idx], c='b', label='Malignant')\n",
    "    plt.scatter(X[y < 0][:, feature_x_idx], X[y < 0][:, feature_y_idx], c='r', label='Benign')\n",
    "\n",
    "    # Create mesh grid for decision boundaries\n",
    "    x_min, x_max = X[:, feature_x_idx].min() - 1, X[:, feature_x_idx].max() + 1\n",
    "    y_min, y_max = X[:, feature_y_idx].min() - 1, X[:, feature_y_idx].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
    "                         np.arange(y_min, y_max, 0.02))\n",
    "    \n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    \n",
    "    # First model's decision boundary (Ground Truth) in orange with highlight effect (thicker and transparent)\n",
    "    Z1 = np.dot(grid, w_1[[feature_x_idx, feature_y_idx]]) + b_1\n",
    "    Z1 = Z1.reshape(xx.shape)\n",
    "    ground_truth = plt.contour(xx, yy, Z1, colors='purple', levels=[0], alpha=0.3, linewidths=5, linestyles=['-'])\n",
    "    plt.contour(xx, yy, Z1, colors='purple', levels=[-1, 1], alpha=0.3, linewidths=3, linestyles=['--'])\n",
    "    \n",
    "    # Second model's decision boundary (ODE Solution) in purple\n",
    "    Z2 = np.dot(grid, w_2[[feature_x_idx, feature_y_idx]]) + b_2\n",
    "    Z2 = Z2.reshape(xx.shape)\n",
    "    ode_solution = plt.contour(xx, yy, Z2, colors='green', levels=[0], alpha=0.8, linewidths=2, linestyles=['-'])\n",
    "    plt.contour(xx, yy, Z2, colors='green', levels=[-1, 1], alpha=0.8, linewidths=1.5, linestyles=['--'])\n",
    "    \n",
    "    # Add the two main lines to the legend\n",
    "    h1, _ = ground_truth.legend_elements()\n",
    "    h2, _ = ode_solution.legend_elements()\n",
    "    legend_elements = [h1[0], h2[0], \n",
    "                       plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='b', markersize=10),\n",
    "                       plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='r', markersize=10)]\n",
    "    legend_labels = ['ODE Solution', 'PINN Solution', 'Malignant', 'Benign']\n",
    "    plt.legend(legend_elements, legend_labels, loc='upper right')\n",
    "    \n",
    "    # Add labels, title, and show the plot\n",
    "    plt.xlabel(f'mean radius')\n",
    "    plt.ylabel(f'mean concave points')\n",
    "    #plt.title('SVM Decision Boundaries')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_label_positions(positions, min_distance=0.02):\n",
    "    \"\"\"Adjust vertical positions of labels to avoid overlap.\"\"\"\n",
    "    sorted_positions = sorted(enumerate(positions), key=lambda x: x[1])\n",
    "    adjusted_positions = positions.copy()\n",
    "    \n",
    "    for i in range(1, len(sorted_positions)):\n",
    "        curr_idx, curr_pos = sorted_positions[i]\n",
    "        prev_idx, prev_pos = sorted_positions[i-1]\n",
    "        \n",
    "        if curr_pos - prev_pos < min_distance:\n",
    "            adjusted_positions[curr_idx] = prev_pos + min_distance\n",
    "    \n",
    "    return adjusted_positions\n",
    "\n",
    "def plot_ode_system(t_values, y_values):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    # Enable LaTeX rendering\n",
    "    plt.rc('text', usetex=True)\n",
    "    plt.rc('font', family='serif')\n",
    "\n",
    "    # Plot each line and prepare label information\n",
    "    labels = [f'w_{i}' for i in range(1,3)] + ['b']\n",
    "    colors = plt.cm.rainbow(np.linspace(0, 1, len(labels)))\n",
    "    label_positions = []\n",
    "\n",
    "    for i, (label, color) in enumerate(zip(labels, colors)):\n",
    "        line = plt.plot(t_values, y_values[i, :], label=label, color=color)[0]\n",
    "        label_positions.append(y_values[i, -1])\n",
    "\n",
    "    # Adjust label positions\n",
    "    y_min, y_max = np.min(y_values), np.max(y_values)\n",
    "    y_range = y_max - y_min\n",
    "    min_distance = 0.02 * y_range  # 2% of y-range as minimum distance\n",
    "    adjusted_positions = adjust_label_positions(label_positions, min_distance)\n",
    "\n",
    "    # Add adjusted labels with increased font size and space\n",
    "    fontsize = 14  # Adjust the font size here\n",
    "    x_offset = 1  # Space to move the label away from the line\n",
    "    \n",
    "    for i, (label, color, position) in enumerate(zip(labels[:3], colors[:3], adjusted_positions[:3])):\n",
    "        math_label = f\"${label}$\"  # Add space with LaTeX's \\quad\n",
    "        x_end = t_values[-1]\n",
    "        plt.text(x_end + x_offset, position, f' {math_label}', verticalalignment='center', \n",
    "                 horizontalalignment='left', color=color, fontsize=fontsize)\n",
    "\n",
    "    plt.xlabel('Time', fontsize=fontsize)\n",
    "    plt.ylabel('y(t)', fontsize=fontsize)\n",
    "    plt.title('ODE System Solution', fontsize=fontsize)\n",
    "    plt.grid(False)\n",
    "\n",
    "    # Adjust the plot limits to make room for labels\n",
    "    plt.xlim(t_values[0], t_values[-1] * 1.1)  # Extend x-axis by 10%\n",
    "    plt.ylim(y_min - 0.1 * y_range, y_max + 0.1 * y_range)  # Extend y-axis by 10% on both ends\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperplane_similarity(w, b, w_opt, b_opt, alpha=0.5, beta=0.5):\n",
    "    \"\"\"\n",
    "    Computes a metric to measure the similarity between two hyperplanes defined by (w, b) and (w_opt, b_opt).\n",
    "    \n",
    "    Parameters:\n",
    "    - w (np.ndarray): Normal vector of the first hyperplane.\n",
    "    - b (float): Offset of the first hyperplane.\n",
    "    - w_opt (np.ndarray): Normal vector of the optimal hyperplane.\n",
    "    - b_opt (float): Offset of the optimal hyperplane.\n",
    "    - alpha (float): Weight for the angular difference metric. Default is 0.5.\n",
    "    - beta (float): Weight for the offset difference metric. Default is 0.5.\n",
    "    \n",
    "    Returns:\n",
    "    - float: Combined metric value. Closer to 0 indicates more similar hyperplanes.\n",
    "    \"\"\"\n",
    "    # Normalize vectors for angle computation\n",
    "    norm_w = np.linalg.norm(w)\n",
    "    norm_w_opt = np.linalg.norm(w_opt)\n",
    "    \n",
    "    # Handle edge cases for zero vectors\n",
    "    if norm_w == 0 or norm_w_opt == 0:\n",
    "        raise ValueError(\"The normal vector of a hyperplane cannot be a zero vector.\")\n",
    "    \n",
    "    # Compute cosine similarity and angle metric\n",
    "    cos_theta = np.dot(w, w_opt) / (norm_w * norm_w_opt)\n",
    "    angle_metric = 1 - cos_theta\n",
    "    \n",
    "    # Normalize vectors to unit length for offset comparison\n",
    "    w_unit = w / norm_w\n",
    "    w_opt_unit = w_opt / norm_w_opt\n",
    "    \n",
    "    # Compute the offset difference metric\n",
    "    offset_diff = abs(b / norm_w - b_opt / norm_w_opt)\n",
    "    offset_metric = offset_diff / max(abs(b / norm_w), abs(b_opt / norm_w_opt), 1)\n",
    "    \n",
    "    # Combine metrics\n",
    "    combined_metric = alpha * angle_metric + beta * offset_metric\n",
    "    return combined_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CVX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "C = 10\n",
    "\n",
    "num_features = selected_X.shape[1]\n",
    "\n",
    "w = cp.Variable(num_features)\n",
    "b = cp.Variable(1)\n",
    "xi = cp.Variable(selected_y.shape[0])\n",
    "\n",
    "objective = cp.Minimize((1/2)*cp.norm(w, 2)**2 + C * cp.sum(xi))\n",
    "\n",
    "constraints = [\n",
    "    cp.multiply(selected_y, cp.matmul(selected_X, w) + b) >= 1 - xi,\n",
    "    xi >= 0\n",
    "]\n",
    "\n",
    "problem = cp.Problem(objective, constraints)\n",
    "\n",
    "t0 = time.time()\n",
    "problem.solve(solver='SCS')\n",
    "solve_time = time.time() - t0\n",
    "\n",
    "# Get solver stats\n",
    "solver_stats = problem.solver_stats\n",
    "print('Time:', solve_time)\n",
    "print('Optimal objective value:', problem.value)\n",
    "print('SCS solver iterations:', solver_stats.num_iters)\n",
    "print('SCS solve time:', solver_stats.solve_time)\n",
    "print('SCS setup time:', solver_stats.setup_time)\n",
    "\n",
    "# Get the values of the variables\n",
    "print('Optimal w: ', w.value)\n",
    "print('Optimal b: ', b.value)\n",
    "\n",
    "# Get the dual variables\n",
    "dual_variables = constraints[0].dual_value\n",
    "print('Dual variables (Lagrange multipliers):', dual_variables)\n",
    "\n",
    "g_1 = [-selected_y[i] * (w.value @ selected_X[i] + b.value) + 1 for i in range(selected_y.shape[0])]\n",
    "g_2 = [np.array([-xi.value[i]]) for i in range(selected_y.shape[0])]\n",
    "g_SCS = jnp.array(g_1 + g_2).flatten()\n",
    "print('Constraints:', g_SCS)\n",
    "\n",
    "# Plotting\n",
    "plot_svm_decision_boundary(selected_X, selected_y, w.value, b.value, 0, 1)\n",
    "\n",
    "wb_cvx = np.concatenate([w.value, b.value])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nw = selected_X.shape[1]\n",
    "nb = 1\n",
    "nxi = selected_y.shape[0]\n",
    "nx = nw + nb + nxi\n",
    "nu = selected_X.shape[0] * 2\n",
    "NY = nx+nu\n",
    "\n",
    "def f(x):\n",
    "    w = x[:nw]\n",
    "    b = x[nw]\n",
    "    xi = x[nw+nb:]\n",
    "    \n",
    "    output = (1/2) * jnp.linalg.norm(w, 2)**2 + C * jnp.sum(xi)\n",
    "    return output\n",
    "\n",
    "def g(x):\n",
    "    w = x[:nw]\n",
    "    b = x[nw]\n",
    "    xi = x[nw+nb:]\n",
    "    \n",
    "    g_1 = [-selected_y[i] * (w @ selected_X[i] + b) + 1 - xi[i] for i in range(selected_y.shape[0])]\n",
    "    g_2 = [- xi[i] for i in range(selected_y.shape[0])]\n",
    "    output = jnp.array(g_1 + g_2)\n",
    "    return output\n",
    "\n",
    "df = fgrad(f) # jacobian of f\n",
    "dg = jacrev(g) # jacobian of g\n",
    "\n",
    "def P(xu): # projection mapping\n",
    "    return xu\n",
    "\n",
    "def ODE(t, xu):\n",
    "    x, u = xu[:nx].reshape((nx, )), xu[nx:].reshape((nu, ))\n",
    "\n",
    "    dx = df(x) + dg(x).T@jnp.clip(u+g(x), a_min=0)\n",
    "    dx = -dx\n",
    "    du = -u + jnp.clip(u+g(x), a_min=0)\n",
    "    du = 0.5*du\n",
    "    dxu = jnp.concatenate([dx, du], axis=0)\n",
    "    return dxu\n",
    "\n",
    "ODE = jit(ODE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evalutation(xu): # metric OuC(x_pred): check if inisde feasable region and return the objective value\n",
    "    \"return  f(x),    if g(x)<=0, f(x)=np.inf, if g(x)>0 \"\n",
    "    x, u= xu[:nx], xu[nx:]\n",
    "    if (g(x)<=0.01).all():\n",
    "        obj = f(x)\n",
    "        #print('g(x) values:', g(x))\n",
    "    else:\n",
    "        obj = np.inf\n",
    "        #print('g(x) values:', g(x))\n",
    "    return obj\n",
    "\n",
    "def evaluation_plus(xu):\n",
    "    x, u= xu[:nx], xu[nx:]\n",
    "    lim = None\n",
    "    if (g(x)<=0.001).all():\n",
    "        lim = 0.001\n",
    "    elif (g(x)<=0.01).all():\n",
    "        lim = 0.01\n",
    "    elif (g(x)<=0.1).all():\n",
    "        lim = 0.1\n",
    "    elif (g(x)<=1).all():\n",
    "        lim = 1\n",
    "    return f(x), lim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_ODE(xuv0, T):\n",
    "    t0 = time.time()\n",
    "    sol = solve_ivp(ODE, [0., T], xuv0, method=NUMERICAL_METHOD)\n",
    "    solve_time = time.time() - t0\n",
    "    print('Time:', solve_time)\n",
    "    result = P(sol.y[:, -1])\n",
    "    \n",
    "    obj, lim = evaluation_plus(result)\n",
    "    print(\"objective:\", obj, \"lim:\", lim)\n",
    "    \n",
    "    print(\"objective:\", obj)\n",
    "    print(\"w: \", result[:nw], \"b: \", result[nw])\n",
    "    wb_ode = result[:nw+nb]\n",
    "    similarity = hyperplane_similarity(result[:nw], result[nw], wb_cvx[:nw], wb_cvx[nw])\n",
    "    print(\"hyperplane similarity:\", similarity)\n",
    "    print(\"Dual variables (alpha):\", result[:nx])\n",
    "    print(\"Constraints:\",g(result[:nx]))\n",
    "    \n",
    "    return sol, result, obj, lim, similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PINN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNN(nn.Module):\n",
    "    def __init__(self, y0):\n",
    "        self.y0 = torch.tensor(y0, dtype=torch.float)\n",
    "        if CUDA:\n",
    "            self.y0 = self.y0.cuda()\n",
    "        super(FNN, self).__init__()\n",
    "        self.linear1 = nn.Linear(1, NEURONS).cuda()\n",
    "        self.linearL = []\n",
    "        for _ in range(LAYERS-1):\n",
    "            self.linearL.append(nn.Linear(NEURONS, NEURONS).cuda())\n",
    "        self.linear2 = nn.Linear(NEURONS, NY).cuda()\n",
    "\n",
    "    def forward(self, x):\n",
    "        t = x.cuda()\n",
    "        x = torch.tanh(self.linear1(x))\n",
    "        for i in range(LAYERS-1):\n",
    "            x = torch.tanh(self.linearL[i](x))\n",
    "        x = self.linear2(x)\n",
    "        x = self.y0 + (1 - torch.exp(-(t-0)))*x\n",
    "        return x\n",
    "\n",
    "\n",
    "class NN_NOP:\n",
    "    def __init__(self, y0, time_range, P, ODE, C_epsilon): # P: projection mapping, ODE, C_epsilon: OuC metric\n",
    "        self.y0 = np.array(y0)\n",
    "        self.time_range = time_range # [0, 10]\n",
    "        self.T = time_range[-1] # T=10\n",
    "        self.P = P\n",
    "        self.ODE = ODE\n",
    "        self.C_epsilon = C_epsilon\n",
    "        self.vODE = vmap(ODE, in_axes=(0, 0), out_axes=0)\n",
    "        #self.numerical_method() # solve analyicallly the ODE\n",
    "\n",
    "    def numerical_method(self):\n",
    "        sol = solve_ivp(self.ODE, self.time_range, self.y0, method=NUMERICAL_METHOD)\n",
    "        self.y_T_ODE = sol.y[:, -1] # get last value of the solution\n",
    "        self.y_T_ODE = self.P(self.y_T_ODE) # project it\n",
    "        self.y_T_ODE = np.array(self.y_T_ODE)\n",
    "        self.epsilon_ODE = self.C_epsilon(self.y_T_ODE) # calculate OuC (f(x))\n",
    "\n",
    "    def NN_method(self, weights=None, t_L=None):\n",
    "        \"\"\"\n",
    "        train the nn, store the following four\n",
    "\n",
    "        L_loss:  list, loss of batch during training\n",
    "        L_epsilon: list, epsilon_best during training\n",
    "        L_y_T: list, yT during training\n",
    "        net_best:  the best neural network model during training\n",
    "        \"\"\"\n",
    "        net = FNN(self.y0)\n",
    "        if weights is not None:\n",
    "            net.load_state_dict(weights.state_dict())\n",
    "        if CUDA:\n",
    "            net = net.cuda()\n",
    "            \n",
    "        optimizer = Adam(net.parameters(), lr=LR) # ========> 0.001 not looks good\n",
    "        i = 0\n",
    "        L_loss = []\n",
    "        L_epsilon = []\n",
    "        L_epsilon_curr = []\n",
    "        L_y_T = []\n",
    "        L_y_T_curr = []\n",
    "        #L_y_iterations = []\n",
    "        t0 = time.time()\n",
    "        while True:\n",
    "            # Update, L_epsilon, L_yT, net_best\n",
    "            T = self.T*torch.ones((1, 1), dtype=torch.float)\n",
    "            if CUDA:\n",
    "                T = T.cuda()\n",
    "            y_T = self.P_multiple(T, net) # get the output of the nn at T (last time)\n",
    "            L_y_T_curr.append(y_T)\n",
    "\n",
    "            epsilon_current = self.C_epsilon(y_T) # calculate OuC (f(x))\n",
    "            L_epsilon_curr.append(epsilon_current)\n",
    "            if i==0: # if first iteration\n",
    "                epsilon_best = epsilon_current\n",
    "                y_T_best = y_T\n",
    "                #y_T_error = np.linalg.norm(self.y_T_ODE-y_T_best, ord=2)\n",
    "                net_best = deepcopy(net)\n",
    "                #L_y_iterations.append(self.y_iteration(net_best))\n",
    "            # if i==10:\n",
    "            #     L_y_iterations.append(self.y_iteration(net_best))\n",
    "            # if i==20:\n",
    "            #     L_y_iterations.append(self.y_iteration(net_best))\n",
    "            # if i==ITERATIONS-1:\n",
    "            #L_y_iterations.append(self.y_iteration(net_best)) # store the output of the best nn across the time\n",
    "            if epsilon_current < epsilon_best:\n",
    "                epsilon_best = epsilon_current\n",
    "                y_T_best = y_T\n",
    "                #y_T_error = np.linalg.norm(self.y_T_ODE-y_T_best, ord=2)\n",
    "                net_best = deepcopy(net)\n",
    "            L_epsilon.append(epsilon_best) # store the best OuC\n",
    "            L_y_T.append(y_T_best) # store the best output of the nn at T\n",
    "\n",
    "            # TRAIN MODEL\n",
    "            # Prepare input t, torch.tensor (NBATCH, 1)\n",
    "            if t_L is not None:\n",
    "                t = t_L[i]\n",
    "            else:\n",
    "                t = np.random.uniform(0, self.T, (NBATCH, 1))\n",
    "            t = torch.tensor(t, dtype=torch.float, requires_grad=True)\n",
    "            if CUDA:\n",
    "                t = t.cuda()\n",
    "            # Learning, L_loss\n",
    "            loss = self.loss_compute(t, net)\n",
    "            loss.backward()\n",
    "            L_loss.append(loss.item())\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            # Monitor\n",
    "            if i%100==0:\n",
    "            # if i in [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 20, 30, 40, 50, 60, 70, 80, 90, 99]:\n",
    "                #print(f'T: {self.T}, y_T_ODE: {self.y_T_ODE.round(4)}, epsilon_ODE: {self.epsilon_ODE :.4f},  Iteration: {i}, Loss: {loss.item() :.4f},  epsilon_best: {epsilon_best :.4f}, y_T: {y_T_best.round(4)}, ||y_T_ODE-y_T_best||_2: {y_T_error :.4f}')\n",
    "                #print(f'T: {self.T}, Iteration: {i}, Loss: {loss.item() :.4f},  epsilon_best: {epsilon_best :.4f}, y_T: {y_T_best.round(4)}')\n",
    "                pass\n",
    "            i = i+1\n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            if i >= 1000 and all(x < 0.01 for x in L_loss[-5:]):\n",
    "                break\n",
    "            \n",
    "            if i==ITERATIONS:\n",
    "                break\n",
    "\n",
    "        curr_net = deepcopy(net)\n",
    "        return L_epsilon, L_epsilon_curr, L_y_T, L_y_T_curr, net_best, curr_net, L_loss, time.time()-t0, i\n",
    "\n",
    "    def loss_compute(self, t, net):\n",
    "        # nn output xu, torch.tensor (NBATCH, nxu)\n",
    "        y = net(t)\n",
    "\n",
    "        # => Get ODE derivatives\n",
    "        # True dxu, torch.tensor (NBATCH, nxu)\n",
    "        t_cpu = t.cpu().detach().numpy()\n",
    "        y_cpu = y[:, :].cpu().detach().numpy()\n",
    "        dy = self.vODE(t_cpu, y_cpu)\n",
    "        dy = np.array(dy)\n",
    "        \n",
    "        dy = torch.tensor(dy, dtype=torch.float)\n",
    "        if CUDA:\n",
    "            dy = dy.cuda()\n",
    "\n",
    "        # => Get NN derivatives (autograd)\n",
    "        # Predicted pdxu, torch.tensor (NBATCH, nxu)\n",
    "        pdy = []\n",
    "        for j in range(NY): # for all outputs node (for all variables)\n",
    "            # y[:, [j]]: outputs of the nn for the j-th variable (all t of the batch), t: inputs of the nn, grad_outputs: scale the gradients to facilitate the compuation of the jacobian, Craete_graph: to compute higer order derivatives\n",
    "            pdyi = grad(y[:, [j]], t, grad_outputs=torch.ones_like(y[:, [j]]), create_graph=True)[0] # torch autograd shape [NBATCH, 1]\n",
    "            pdy.append(pdyi)\n",
    "        pdy = torch.cat(pdy, dim=1) # for makning it a tensor: [NBATCH, NY]\n",
    "        #pdy = torch.cat(pdy, dim=1) # for makning it a tensor: [NBATCH, NY]\n",
    "\n",
    "        # Compute loss, torch.tensor a float\n",
    "        dynamics_loss = mse(dy, pdy) # [NBATCH, NY]\n",
    "        dynamics_loss = torch.sum(dynamics_loss, axis=1) # [NBATCH]\n",
    "        dynamics_loss = torch.exp(-WEIGTHING*(t / T).detach().reshape((-1, )))*dynamics_loss  # proprity smaller t\n",
    "        dynamics_loss = torch.mean(dynamics_loss, axis=0)\n",
    "        \n",
    "        return dynamics_loss\n",
    "\n",
    "    def P_multiple(self, t, net): # get the projection of the output of the nn (for multiple t)\n",
    "        y = net(t)\n",
    "        y = y.reshape((-1, )).cpu().detach().numpy()\n",
    "        output = self.P(y)\n",
    "        output = np.array(output)\n",
    "        return output\n",
    "\n",
    "    def y_iteration(self, net): # get the ODE system estiamtion across the time\n",
    "        # For L_y\n",
    "        t = torch.arange(0, self.T, 0.1).reshape((-1, 1))\n",
    "        if CUDA:\n",
    "            t = t.cuda()\n",
    "        y = [self.P_multiple(t[i].reshape((1,1)), net) for i in range(t.shape[0])]\n",
    "        y = torch.tensor(np.array(y)).cpu().detach().numpy()\n",
    "\n",
    "        return y # [T, NY]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iteration(ip):\n",
    "    xu0 = jnp.array(ip)\n",
    "    y0 = ip\n",
    "    \n",
    "    # ODE Solution\n",
    "    sol, result, obj, lim, similarity = solve_ODE(xu0, T)\n",
    "    \n",
    "    if lim == None:\n",
    "        print('>>> Infeasible')\n",
    "    else:\n",
    "        print('>>> Feasible with epsilon:', lim, 'objective:', obj)\n",
    "\n",
    "    print(\"hyperplane similarity:\", similarity)\n",
    "    if similarity < 0.01:\n",
    "        print('>>> ODE Solution is accurate, last iteration')\n",
    "        lastIter = True\n",
    "    else:\n",
    "        lastIter = False\n",
    "        \n",
    "    t_values = sol.t\n",
    "    y_values = sol.y\n",
    "\n",
    "    w_ode = result[:data_X.shape[1]]\n",
    "    b_ode = result[data_X.shape[1]]\n",
    "\n",
    "    plot_ode_system(t_values, y_values)\n",
    "\n",
    "    plot_svm_decision_boundary_confrontation(selected_X, selected_y, w.value, b.value, w_ode, b_ode, 0, 1)\n",
    "    \n",
    "    example = NN_NOP(y0, [0, T], P, ODE, evalutation) # init\n",
    "    L_epsilon, L_epsilon_curr, L_y_T, L_y_T_curr, net_best, net_curr, L_loss, t, n_iter = example.NN_method() # train\n",
    "    print('epsilon:', L_epsilon[-1], 'time:', t, 'iterations:', n_iter)\n",
    "    \n",
    "    # ------------ NN plots ------------ \n",
    "    plt.plot(L_epsilon, marker='o', linestyle='-', label='OuC metric best', markersize=1)\n",
    "    plt.plot(L_epsilon_curr, marker='o', linestyle='-', label='OuC metric current', markersize=1)\n",
    "    plt.xlabel('iteration')\n",
    "    plt.ylabel('OuC metric')\n",
    "    #plt.title(f'l={res_1[\"layers\"]}, n={res_1[\"neurons\"]}, lr={res_1[\"lr\"]}')\n",
    "    plt.legend()\n",
    "    plt.grid(False)\n",
    "    #plt.set_ylim(y_min, y_max)\n",
    "    plt.xlim(0, ITERATIONS)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "    \n",
    "    print('Last losses:', L_loss[-10:])\n",
    "\n",
    "    plt.plot(L_loss, label='Loss')\n",
    "    plt.xlabel('iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    #plt.title(f'l={res_1[\"layers\"]}, n={res_1[\"neurons\"]}, lr={res_1[\"lr\"]}')\n",
    "    plt.legend()\n",
    "    plt.grid(False)\n",
    "    #plt.set_ylim(y_min, y_max)\n",
    "    plt.xlim(0, ITERATIONS)\n",
    "    plt.tight_layout()\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "    \n",
    "    if n_iter != ITERATIONS and lastIter:\n",
    "        print('>>> NN Solution is accurate, last iteration')\n",
    "        net = net_curr\n",
    "        lastIter = True\n",
    "    else:\n",
    "        lastIter = False\n",
    "        net = net_curr\n",
    "    \n",
    "    res = net(torch.tensor([T]).cuda()).cpu().detach().numpy()\n",
    "    plot_svm_decision_boundary_confrontation_PINN(selected_X, selected_y, w_ode, b_ode, res[:data_X.shape[1]], res[data_X.shape[1]], 0, 1)\n",
    "    print('Variables:', res[:nx])\n",
    "    print('Constraints:', g(res[:nx]))\n",
    "    \n",
    "    pinn_obj, lim_pin = evaluation_plus(res)\n",
    "    \n",
    "    print(\"objective pinn:\", pinn_obj, \"epsion pinn:\", lim_pin)\n",
    "    \n",
    "    print(\"w: \", res[:nw], \"b: \", res[nw])\n",
    "\n",
    "    similarity_pinn = hyperplane_similarity(res[:nw], res[nw], wb_cvx[:nw], wb_cvx[nw])\n",
    "    print(\"PINN hyperplane similarity:\", similarity_pinn)\n",
    "    \n",
    "    return res, pinn_obj, lastIter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NBATCH = 512\n",
    "WEIGTHING = 1\n",
    "LAYERS = 2\n",
    "NEURONS = 100\n",
    "LR = 0.001\n",
    "T = 20.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITERATIONS = 2000\n",
    "LR = 0.01\n",
    "ip = np.zeros(NY)\n",
    "ip[:nw + nb] = 1.0\n",
    "\n",
    "for i in range(8):\n",
    "    print('>>>> ITERATION:', i+1)\n",
    "    res, pinn_obj, lastIter = iteration(ip)\n",
    "    wb_pinn = res[:nw+1]\n",
    "    if lastIter:\n",
    "        print('>>>> Results obtained, OuC: ', pinn_obj)\n",
    "        break\n",
    "    ip = res\n",
    "    ITERATIONS = 10000\n",
    "    LR = 0.0001"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
